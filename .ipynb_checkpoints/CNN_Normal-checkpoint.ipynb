{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78414749-44f8-4a58-a445-0486de6e4dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision, torch, glob\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sam.sam import SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b701d-268b-4ee6-9fd6-c96ea6dac16b",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51ed73c-e3d8-4d32-9515-ad4c4a0ccc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_dict = {\n",
    "        'train': transforms.Compose(\n",
    "            [transforms.CenterCrop(256),\n",
    "             transforms.RandomHorizontalFlip(),\n",
    "             transforms.ToTensor(),\n",
    "             ]),\n",
    "        'test': transforms.Compose(\n",
    "            [transforms.CenterCrop(256),\n",
    "             transforms.RandomHorizontalFlip(),\n",
    "             transforms.ToTensor(),\n",
    "             ])}\n",
    "\n",
    "data_folder = \"./dataset2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf521ea-cf5b-47fe-9814-e265cb3d90b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torchvision.datasets.ImageFolder(root=data_folder, transform=transform_dict['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2f8c3-1a52-44c9-902d-21d13250b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data) * 10 / 11)\n",
    "valid_size  = len(data) - train_size      \n",
    "data_size  = {\"train\":train_size, \"valid\":valid_size}\n",
    "print(data_size)\n",
    "data_train, data_valid = torch.utils.data.random_split(data, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a688f-b5a3-4542-9feb-ba2b4b0c3a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "valid_loader   = torch.utils.data.DataLoader(data_valid, batch_size=1, shuffle=False)\n",
    "dataloaders  = {\"train\":train_loader, \"valid\":valid_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad935d-45a7-45f1-a858-bec2e01dfbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     \n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# 訓練データをランダムに取得\n",
    "dataiter = iter(dataloaders[\"train\"])\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# 画像,ラベルの表示\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(' '.join('%5s' % labels[j] for j in range(batch_size)))\n",
    "\n",
    "#1:cloudy, 2:rain, 3:shine, 4:sunrise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb46f37-1754-463d-8533-c16832211d17",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc40bbc7-c3fb-4ecc-964a-cbf54b5da743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size =2), nn.LeakyReLU(),\n",
    "            nn.Conv2d(in_channels = 32, out_channels = 128, kernel_size =2), nn.LeakyReLU(), \n",
    "            nn.Conv2d(in_channels = 128, out_channels = 512, kernel_size =2), nn.LeakyReLU(),\n",
    "        )\n",
    "        self.attn_conv = nn.Sequential(\n",
    "            nn.Conv2d(512, 4, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048,5), nn.Softmax(dim=1),\n",
    "        )\n",
    "    def forward(self, img):\n",
    "        x1 = self.conv(img)\n",
    "        attn = self.attn_conv(x1)\n",
    "        B, A, H, W = attn.shape\n",
    "        \n",
    "        attn = attn.reshape(B,A,1,H,W)\n",
    "        x1 = x1.reshape(B,1,512,H,W)\n",
    "        x2 = x1 * attn\n",
    "        x2 = x2.reshape(B*A,512,H,W)\n",
    "        \n",
    "        x3 = F.adaptive_avg_pool2d(x2, (1,1))\n",
    "        x4 = x3.reshape(B,-1)\n",
    "        x5 = self.fc(x4)\n",
    "        return x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16105e24-3369-400d-8fa0-6561a3ea6981",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 20\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AttentionCNN()\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "#SAM\n",
    "base_optimizer = torch.optim.SGD \n",
    "optimizer = SAM(model.parameters(), base_optimizer, lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40cab2c-d879-4e66-b8cb-18895a173880",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "for i in range(epoch):\n",
    "    train_running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    for images, labels in dataloaders[\"train\"]:\n",
    "        images = images.to(device)\n",
    "        labels = torch.nn.functional.one_hot(labels, num_classes=5)\n",
    "        labels = labels.to(device).to(torch.float32)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "        \n",
    "        loss_func(model(images),labels).backward()  \n",
    "        optimizer.second_step(zero_grad=True)\n",
    "        \n",
    "        train_running_loss += loss.item()\n",
    "    if i % (epoch/10) == 0:\n",
    "        score = 0\n",
    "        for images, labels in dataloaders[\"valid\"]:\n",
    "            images = images.to(device)\n",
    "            labels = torch.nn.functional.one_hot(labels, num_classes=5)\n",
    "            labels = labels.to(device).to(torch.float32)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "    \n",
    "            valid_running_loss += loss.item()\n",
    "            \n",
    "            label = torch.argmax(labels[0])\n",
    "            output = torch.argmax(outputs[0])\n",
    "            if label == output :\n",
    "                score += 1\n",
    "        \n",
    "        train_running_loss /= 102\n",
    "        valid_running_loss /= (10.2*batch_size)\n",
    "        train_loss_list.append(train_running_loss)\n",
    "        valid_loss_list.append(valid_running_loss)\n",
    "        print(\"Epoch:{}, Train_Loss:{}, Valid_Loss:{}\".format(i, train_running_loss, valid_running_loss))\n",
    "        print(\"Acc:\",score/102)\n",
    "        \n",
    "plt.yscale('log')\n",
    "plt.plot(np.arange(len(train_loss_list)), train_loss_list, label=\"train\")\n",
    "plt.plot(np.arange(len(valid_loss_list)), valid_loss_list, label=\"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5e989c-9dbe-4c44-91a3-c0e5be8a84a5",
   "metadata": {},
   "source": [
    "#Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5629fd-42b6-469c-97d7-f670b6dba7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0\n",
    "i = 0\n",
    "Total = 102\n",
    "for images, labels in dataloaders[\"valid\"]:\n",
    "            i += 1\n",
    "            image = images.to(device)\n",
    "            label = F.one_hot(labels, num_classes=5)\n",
    "            label = label.to(device).to(torch.float32)\n",
    "            \n",
    "            output = model(image)\n",
    "            Label = torch.argmax(label[0])\n",
    "            Output = torch.argmax(output[0])\n",
    "            if Label == Output :\n",
    "                score += 1\n",
    "            else:\n",
    "                print(\"Valid:\", label[0])\n",
    "                print(\"Predict:\",output[0])\n",
    "print(\"Acc:\", score/Total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd4525-01a1-4287-a641-6c98637d2d0d",
   "metadata": {},
   "source": [
    "# こだわりポイント\n",
    "・SAMっていうoptimizer使った.めっちゃSoTA更新しているらしい!理論説明は頼んだ！ OK  \n",
    "・attention使った。オリジナルで作ったからあんまりうまく行ってないかも。理解できたらViTransformer試したみたい...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f4945-5bbe-46a9-b00d-f8d129234096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
